{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3903c9-dea8-4342-bdd7-ff6a64f925e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\chris\\anaconda3\\lib\\site-packages (7.7.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b3780d-808f-4eb8-ba67-3888a36a30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.1 of praw is outdated. Version 7.8.1 was released 4 days ago.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose a subreddit (don't type r/singapore - singapore will do):  singapore\n",
      "key the post ID:  1f8fccz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to 8x9fytckpomd1.jpeg\n",
      "Comments and replies extracted and saved to 'url_scraper_reddit_api_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set up Reddit API credentials\n",
    "client_id = 'Bkulv7ePr58Vu37vLBObAg'\n",
    "client_secret = 'jJTaqSbHFW-BXSYh0Br4copXSsiSWw'\n",
    "user_agent = 'Mozilla/5.0'\n",
    "\n",
    "# Initialize the PRAW Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "inSubreddit = input(\"choose a subreddit (don't type r/singapore - singapore will do): \")\n",
    "inPostID = input(\"key the post ID: \")\n",
    "\n",
    "# Define the Reddit post URL\n",
    "post_url = 'https://www.reddit.com/r/' + inSubreddit + '/comments/' + inPostID + '/'  # Replace 'post_id' with the actual ID of the post\n",
    "\n",
    "# Get the submission (post) object\n",
    "submission = reddit.submission(url=post_url)\n",
    "\n",
    "# Function to download an image\n",
    "def download_image(image_url, save_path):\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Image saved to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image from {image_url}\")\n",
    "\n",
    "    # Check if the post contains an image\n",
    "if submission.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "    # If the post URL itself points to an image\n",
    "    image_url = submission.url\n",
    "    # Define the local filename to save the image\n",
    "    file_name = image_url.split(\"/\")[-1]  \n",
    "    download_image(image_url, file_name)\n",
    "\n",
    "elif hasattr(submission, 'preview') and 'images' in submission.preview:\n",
    "    # If the post has an image in the preview attribute\n",
    "    image_url = submission.preview['images'][0]['source']['url']\n",
    "    # Define the local filename to save the image\n",
    "    file_name = image_url.split(\"/\")[-1]\n",
    "    download_image(image_url, file_name)\n",
    "\n",
    "elif hasattr(submission, 'media_metadata'):\n",
    "    # If the post contains a gallery or multiple images\n",
    "    for media_id, media in submission.media_metadata.items():\n",
    "        image_url = media['s']['u']\n",
    "        # Define the local filename to save the image\n",
    "        file_name = f\"{media_id}.jpg\"  \n",
    "        download_image(image_url, file_name)\n",
    "\n",
    "else:\n",
    "    print(\"No image found in the post.\")\n",
    "\n",
    "# Initialize a list to store comments and replies\n",
    "comments_dataset = []\n",
    "\n",
    "# Function to recursively extract comments and replies\n",
    "def extract_comments(comment, parent_id=None):\n",
    "    # Add the current comment to the dataset\n",
    "    comments_dataset.append({\n",
    "        'comment_id': comment.id,\n",
    "        'parent_id': parent_id,\n",
    "        'comment_author': comment.author.name if comment.author else 'deleted',\n",
    "        'comment_body': comment.body,\n",
    "        'comment_score': comment.score\n",
    "    })\n",
    "    \n",
    "    # Recursively extract replies\n",
    "    for reply in comment.replies:\n",
    "        extract_comments(reply, parent_id=comment.id)\n",
    "\n",
    "# Fetch all top-level comments and extract their replies\n",
    "submission.comments.replace_more(limit=None)  # Ensure all comments are loaded\n",
    "for top_level_comment in submission.comments.list():\n",
    "    extract_comments(top_level_comment)\n",
    "\n",
    "# Convert the comments dataset to a DataFrame\n",
    "df_comments = pd.DataFrame(comments_dataset)\n",
    "\n",
    "# Save the comments and replies data to a CSV file\n",
    "df_comments.to_csv('url_scraper_reddit_api_dataset.csv', index=False)\n",
    "\n",
    "print(\"Comments and replies extracted and saved to 'url_scraper_reddit_api_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e584eb-11d5-429f-8871-542efc38f1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
